# LABORATORY-WORK.-DEVELOPING-A-MACHINE-LEARNING-SYSTEM
Отчет по лабораторной работе #2 выполнил(а):
- Фоменко Андрей Васильевич
- РИ000024
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
Познакомиться с программными средствами для организции
передачи данных между инструментами google, Python и Unity

## Задание 1
### Реализовать совместную работу и передачу данных в связке Python
- Google-Sheets – Unity. При выполнении задания используйте видео-материалы и
исходные данные, предоставленные преподавателя курса.

Ход работы:

```py
import gspread
import numpy as np

gc = gspread.service_account(filename='unitydatasciense-364409-28441679f719.json')
sh = gc.open("UnitySheets")
price = np.random.randint(2000, 10000, 11)
mon = list(range(1, 11))
i = 0
while i <= len(mon):
    i += 1
    if i == 0:
        continue
    else:
        tempInf = ((price[i - 1] - price[i - 2]) / price[i - 2]) * 100
        tempInf = str(tempInf)
        # tempInf = tempInf.replace('.' , ',')
        sh.sheet1.update(('A' + str(i)), str(i))
        sh.sheet1.update(('B' + str(i)), str(price[i-1]))
        sh.sheet1.update(('C' + str(i)), str(tempInf))
        print(tempInf)

```

## Задание 2
### Подробно опишите каждую строку файла конфигурации нейронной сети, доступного в папке с файлами проекта по ссылке. Самостоятельно найдите информацию о компонентах Decision Requester, Behavior Parameters, добавленных на сфере.


```py
behaviors:       
  RollerBall:
    **trainer_type: ppo ** # (по умолчанию = ppo) Тип используемого тренажера: ppo, sac или poca.
    
    hyperparameters:
    
      **batch_size: 10 ** # Количество опытов в каждой итерации градиентного спуска. Это всегда должно быть в несколько раз меньше, чем размер буфера. Если вы используете непрерывные действия, это значение должно быть большим (порядка 1000 с). Если вы используете только дискретные действия, это значение должно быть меньше (порядка 10 с). Типовой диапазон: (непрерывный - PPO): 512 - 5120; (непрерывный - SAC): 128 - 1024; (Дискретный, PPO и SAC): 32–512.
      
      **buffer_size: 100** #(по умолчанию = 10240 для PPO и 50000 для SAC)
PPO: количество опытов, которые необходимо собрать перед обновлением модели политики. Соответствует тому, сколько опыта должно быть собрано, прежде чем мы будем изучать или обновлять модель. Это значение должно быть в несколько раз больше, чем размер_пакета. Обычно больший размер буфера соответствует более стабильным обновлениям обучения.
SAC: максимальный размер буфера опыта — примерно в тысячи раз больше, чем ваши эпизоды, чтобы SAC мог учиться как на старом, так и на новом опыте. Типовой диапазон: PPO: 2048 - 409600; САК: 50000 - 1000000
      
      **learning_rate: 3.0e-4** # Начальная скорость обучения для градиентного спуска. Соответствует силе каждого шага обновления градиентного спуска. Обычно это значение следует уменьшать, если обучение нестабильно, а вознаграждение не увеличивается постоянно. Типичный диапазон: 1e-5 - 1e-3
      
      **beta: 5.0e-4** # (по умолчанию = 5.0e-3) Сила регуляризации энтропии, которая делает политику «более случайной». Это гарантирует, что агенты должным образом исследуют пространство действия во время обучения. Увеличение этого параметра обеспечит выполнение большего количества случайных действий. Это должно быть скорректировано таким образом, чтобы энтропия (измеряемая с помощью TensorBoard) медленно уменьшалась вместе с увеличением вознаграждения. Если энтропия падает слишком быстро, увеличьте бета. Если энтропия падает слишком медленно, уменьшите бета.Типичный диапазон: 1e-4 -1e-2
      
      **epsilon: 0.2** # (по умолчанию = 0,2) Влияет на скорость изменения политики во время обучения. Соответствует допустимому порогу расхождения между старой и новой политикой при обновлении градиентного спуска. Установка небольшого значения этого параметра приведет к более стабильным обновлениям, но также замедлит процесс обучения. Типичный диапазон: 0,1–0,3
      
      **lambd: 0.99** # (по умолчанию = 0,95) Параметр регуляризации (лямбда), используемый при расчете обобщенной оценки преимущества (GAE). Это можно рассматривать как то, насколько агент полагается на свою текущую оценку стоимости при вычислении обновленной оценки стоимости. Низкие значения соответствуют большему полаганию на текущую оценку ценности (что может быть высоким смещением), а высокие значения соответствуют большему полаганию на фактические вознаграждения, полученные в среде (что может быть высокой дисперсией). Параметр обеспечивает компромисс между ними, и правильное значение может привести к более стабильному процессу обучения. Типичный диапазон: 0,9–0,95
      
      **num_epoch: 3** # (по умолчанию = 3) Количество проходов через буфер опыта при оптимизации градиентного спуска. Чем больше размер партии, тем больше это допустимо. Уменьшение этого параметра обеспечит более стабильные обновления за счет более медленного обучения. Типичный диапазон: 3–10
      
      **learning_rate_schedule: linear** # (по умолчанию = линейный для PPO и постоянный для SAC) Определяет, как скорость обучения изменяется с течением времени. Для PPO мы рекомендуем снижать скорость обучения до значения max_steps, чтобы обучение сходилось более стабильно. Однако в некоторых случаях (например, при обучении в течение неизвестного времени) эту функцию можно отключить. Для SAC мы рекомендуем поддерживать скорость обучения постоянной, чтобы агент мог продолжать обучение до тех пор, пока его функция Q не сойдется естественным образом. linear линейно затухает Learning_rate, достигая 0 при max_steps, в то время как Constant сохраняет скорость обучения постоянной для всего тренировочного прогона.
      
    network_settings:
      **normalize: false** # (по умолчанию = false) Применяется ли нормализация к входным данным векторного наблюдения. Эта нормализация основана на скользящем среднем и дисперсии векторного наблюдения. Нормализация может быть полезна в случаях со сложными задачами непрерывного управления, но может быть вредна для более простых задач дискретного управления.
      
     ** hidden_units: 128** # (по умолчанию = 128) Количество юнитов в скрытых слоях нейронной сети. Соответствуют количеству единиц в каждом полносвязном слое нейронной сети. Для простых задач, где правильное действие представляет собой простую комбинацию входных данных наблюдения, это значение должно быть небольшим. Для задач, где действие представляет собой очень сложное взаимодействие между переменными наблюдения, это значение должно быть больше. Типичный диапазон: 32–512
     
     ** num_layers: 2** # (по умолчанию = 2) Количество скрытых слоев в нейронной сети. Соответствует количеству скрытых слоев после ввода наблюдения или после кодирования CNN визуального наблюдения. Для простых задач меньше слоев, скорее всего, будут обучать быстрее и эффективнее. Для более сложных задач управления может потребоваться больше слоев. Типичный диапазон: 1–3
     
    **reward_signals:** # Раздел вознаграждения_сигналов позволяет указать настройки как для внешних (т. е. основанных на среде), так и для внутренних сигналов вознаграждения (например, любопытство и GAIL). Каждый сигнал вознаграждения должен определять по крайней мере два параметра, силу и гамму, в дополнение к любым гиперпараметрам, специфичным для класса. Обратите внимание: чтобы удалить сигнал вознаграждения, вы должны полностью удалить его запись из вознаграждения_сигналов. По крайней мере, один сигнал вознаграждения должен оставаться определенным в любое время. Предоставьте следующие конфигурации для разработки сигнала вознаграждения для вашего тренировочного прогона.
    
      **extrinsic:** # Включите эти настройки, чтобы ваш тренировочный забег включал ваш сигнал вознаграждения на основе среды:
      
        **gamma: 0.99** # (по умолчанию = 0,99) Фактор скидки для будущих вознаграждений, поступающих из окружающей среды. Это можно рассматривать как то, как далеко в будущем агент должен заботиться о возможных вознаграждениях. В ситуациях, когда агент должен действовать в настоящем, чтобы подготовиться к вознаграждению в отдаленном будущем, это значение должно быть большим. В случаях, когда вознаграждение является более немедленным, оно может быть меньше. Должно быть строго меньше 1. Типичный диапазон: 0,8–0,995
        
        **strength: 1.0** # (по умолчанию = 1.0) Коэффициент, на который умножается вознаграждение, данное средой. Типичные диапазоны будут варьироваться в зависимости от сигнала вознаграждения. Типичный диапазон: 1,00
    
    **max_steps: 500000** # Общее количество шагов (т. е. собранных наблюдений и предпринятых действий), которые необходимо выполнить в среде (или во всех средах при параллельном использовании нескольких) перед завершением процесса обучения. Если в вашей среде есть несколько агентов с одинаковым именем поведения, все шаги, предпринятые этими агентами, будут учитываться в одном и том же счетчике max_steps.
    
    **time_horizon: 64** # Сколько шагов опыта нужно собрать для каждого агента, прежде чем добавить его в буфер опыта. Когда этот предел достигается до конца эпизода, оценка значения используется для прогнозирования общего ожидаемого вознаграждения из текущего состояния агента. Таким образом, этот параметр является компромиссом между менее предвзятой, но более высокой оценкой дисперсии (длинный временной горизонт) и более предвзятой, но менее разнообразной оценкой (короткий временной горизонт). В тех случаях, когда в эпизоде ​​есть частые награды или эпизоды непомерно велики, более идеальным может быть меньшее количество. Это число должно быть достаточно большим, чтобы охватить все важные действия в последовательности действий агента.
    
    **summary_freq: 10000** # Количество опытов, которое необходимо собрать перед созданием и отображением статистики обучения. Это определяет детализацию графиков в Tensorboard.
```
 - **The DecisionRequester component** автоматически запрашивает решения для экземпляра агента через регулярные промежутки времени.

 - **Behavior Parameters** Компонент для настройки поведения экземпляра агента и свойств мозга.



## Выводы

В данной рвботе я научился записывать данные в сервис GoogleSheets с помощью Python, а также считывать данные GoogleSheets по средствам c# скрипта для Unity. 

Самостоятельно разработал сценарий воспроизведения звукового
сопровождения в Unity, пререработав предложенный код под формат - На основании величины Loss происходит звуковое сопроводение выпавшей карты из пака - чем **ниже** величина **loss** тем **выше** ценность,выпавшей карты)

- Звуковые исходники были взяты из игры **Hearthstone**

bb

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**Lampovui (Fomenko Andrey)
